# 2015/02/28

如果链接不上zk，自动进入safemode，从safemode出来之后要判断自己是否是leader，不是的话，重置状态机

迁移过程中，如果机器挂了，迁移前的MigPlan的信息就错了，需要得到新主的信息重新设置标记才能继续
所以迁移和Failover是有交叉逻辑的，而且Failover的优先级比较高，在Failover之后，还要能回到之前的状态

所以状态机实际上是类似中断处理函数

迁移过程中每迁移一个slot或遇到错误时，检查是否有消息

SlaveDieEvent
SlaveRecoverEvent
MasterDieEvent
MasterRecoverEvent
ZkConnLostEvent
ZkMasterChangeEvent

MigrateCommand
FailoverCommand
ReelectCommand
SetMasterCommand
EnterSafeModeCommand
LeaveSafeModeCommand

case MigrateCommand:
if m.TopState().Name() == "RUNNING" {
    m.State == state.MIGRATING
    m.PushState(MigrateTask)
}

StateManager

MigrateState
FailoverState

故障类型：
1. 单机故障
2. 机房故障
3. 地域链路故障
4. 压力过大

如果是写入压力（比如网卡打爆导致严重丢包）过大导致的FAIL判定：
1. 停写
2. 迁移
3. 开启写入

如果是从读压力过大导致：
1. 判断是否是单key问题
2. 如果单key热点，加从或等待
3. 如果非单key热点，

- 核心功能是Failover和扩容缩容，其他功能如集群搭建、日常处理（集群迁移、每日备份、封禁某台
  机器、换Master地域等）是非核心功能，我们考虑可能性，但不急于实现。

  首先，集群主要有三种状态，正常运行、数据迁移、故障处理。优先级为：正常运行 < 数据迁移 < 故
  障处理，在处于这三种状态时，系统还可以接受请求处理其他逻辑，包括查询状态（拓扑、统计数据
  等）、封禁读写、甚至数据迁移。官方Redis的Failover做法很单一，即重新选主，这种方式无法处理
  压力过大引起的问题——换主之后可能仍然会打爆。妥善的做法是在决定采取何种措施之前，搞清楚
  Failure的原因。

  到底状态该放在哪个粒度进行呢？是整个集群一个？是每个分片一个？是每个Server一个？最初想法是
  整个集群一个，但是仔细推敲状态变换后可以发现，当出现一个以上节点挂掉的情况，状态机就很难理清
  了，尤其当故障发生时还处于迁移状态。迁移过程中，某个节点故障需要中断迁移，就需要考虑故障节点
  与正在迁移的节点是否关系：有关系如何处理？没关系如何处理？处理完迁移是否继续？是否能继续？如
  果节点故障是由于写压力太大把网卡打爆丢包严重导致，可能我们还需要先短暂停止Master的写入，迁
  移热点数据到其他节点，然后等待恢复，那么恢复之后呢，之前未完成的迁移操作怎么办？如果有Slot迁
  移到一半怎么办？如果正在迁移的Slot的目标或源节点挂了换了主怎么办？

  不同节点的状态揉在一起问题很复杂，将状态都维护在节点粒度呢，每个节点一个独立的状态机。状态有三
  种：RUNNING,MIGRATING,WAIT_FAILOVER。三者可互相转换。迁移操作按源节点聚合，因为是节点粒度
  做迁移，可以并发，不同源节点向同一个目标节点迁移数据，看起来很合理。当节点故障时，Failover该节
  点即可，因为状态机是节点粒度的，也就不需要考虑其他节点故障对自己的影响了。

  看起来是这样，但问题要复杂一些。考虑一种场景，同一分片，A是Master，B是Slave，A挂了，进行
  FAILOVER，选择B为新主，此时发现B挂了（注意故障发现类似中断，随时可能发生），由于B不知道A将其
  选择为新主了，所以就自行Failover了，对于从故障一般是封禁读流量，B处理结束后，A选择B为新Master，
  但此时B已经挂了，该FAILOVER会失败，需要管理员重新发起Failover操作，但是如果B并不是真的挂了，
  请求可以发送过去呢，则会造成这个新主的读流量被封禁，不过，如果是假死，过一段时间读会恢复，所以
  这个场景没有问题。

  如果某个源节点正在迁移，此时该节点挂了，迁移暂停，状态由MIGRATING切换到WAIT_FAILOVER，管理员
  确认进行Failover，选择新节点B为Master，Failover结束后，需要将迁移操作转移给B继续才对，如何进
  行？有两种方案：(1)显式将迁移任务转移给B，切换B到MAIGRATING状态。(2)将迁移任务设置为分片
  (ReplicaSet)粒度，同一个分片只有一个迁移任务，转移只是设置B为该分片的Master，迁移任务仅仅取
  Master进行迁移操作即可。两个方案都有些细节问题：方案1可能有竞争状态，方案2的MIGRATING状态维护
  是个问题。对于方案1，如果将迁移状态转移给B时，发现B处于WAIT_FAILOVER状态，此时问题变成处于
  WAIT_FAILOVER状态的节点，收到MIGRATE消息时该如何处理，如果迁移可以正常结束，则切换回RUNNING状
  态，如果节点仍然异常，会再次进入WAIT_FAILOVER状态，问题是如果迁移失败怎么办，该进入哪个状态？迁
  移失败的可能性很多，可能是源节点挂了，可能是目标节点挂了，也可能是临时性的失败。

  迁移任务失败解决思路：迁移失败一律回到RUNNING状态，由另一个定时任务定期检查是否是否还有未完成的
  迁移，仅当MigPlan不空且状态为RUNNING时才会切换到MIGRATING状态。对于上面提到的场景，处于
  WAIT_FAILOVER状态后又发起迁移操作进入MIGRATING状态，当迁移失败后，不会回到WAIT_FAILOVER状态，
  而是回到RUNNING状态，由于节点仍未恢复，会重新进入WAIT_FAILOVER状态。

  迁移与Failover真的需要耦合在一起吗。不可以同时处于迁移状态也处于WAIT_FAILOVER状态吗，迁移只做迁
  移，当节点挂了，自然迁移就失败等待重试，同时会进入WAIT_FAILOVER状态。如果目标节点挂了，同样会失败
  等待重试，重试时会检查目标是否有变更，以便正确迁移。迁移可以Cancel，可以Pause，可以Reset，Reset
  (newPlan)用于替换现有迁移任务，如果有的slot进行到一半，需要将其与新任务合并。

  每个节点的运行状态简化为：RUNNING, WAITING_FAILOVER, SWITCHING_MASTER。每个ReplicaSet有一
  个迁移任务，迁移任务在相关节点进行SWITCHING_MASTER过程中处于暂停状态，结束后继续。

### RUNNING状态
(a) Dead消息：
1. 判断是主还是从
2. 如果是从，判断是否可以（CanFailoverSlave）进行自动处理（封禁），如果可以就调用封禁任务。如果不行则进入WAIT_FAILOVER状态。
3. 如果是主，检查是否可以自动Failover（CanFailoverMaster），如果可以则进入SWITCHING_MASTER状态，否则进入WAIT_FAILOVER状态。
(b) Alive消息：
1. 获取Server的读写状态
2. 如果是从，且不可读，检查读封禁列表，如果不在其中，则开启读，否则不操作。
3. 如果是主，且不可读，检查读封禁列表，如果不在其中，则开启读，否则不操作；如果不可写，检查写封禁列表，如果不再其中，则开启写。
(c) SetAsMaster消息：
1. 进入SwitchingMaster状态

### WAIT_FAILOVER状态
(a) Failover消息
检查角色和CanFailoverRole()，如果可以则进行FAILOVER，如果是从则封禁，如果是主，进入SwitchingMaster状态
(b) Dead消息，忽略
(c) Alive消息，转移状态到RUNNING
(d) SetAsMaster消息，报错

### SWITCHING_MASTER状态
忽略所有消息，检查是否进行完


MigrateTask:
Start()
Pause()
Resume()
Reset()
Cancel()
ReplaceSource()
ReplaceTarget()

状态有：
RUNNING
MIGRATING
WAITING_SLAVE_FAILOVER
WAITING_MASTER_FAILOVER
SAFEMODE

简单场景1:
1. 正在扩容，数据迁移中
2. 某台主机挂掉了
处理方案：
1. 人工确认进行AutoFailover
当Failover结束后，继续之前未完成的迁移，正常情况下会出现如下StateStack。但注意，之前的MigPlan已经无效了，因为故障节点就包含在其中，此时我们希望取消迁移。有两种情形需要考虑：
1. 如果Migrate被打断在slot迁移之间，可以直接结束。
2. 如果Migrate被打断在slot迁移之内，且该slot与故障节点无关，我们需要继续将该slot迁移完成，因为redis上还设置着迁移状态标记，并且同一个slot的数据分散在两台机器上。
3. 如果Migrate被打断在slot迁移之内，且该slot与故障节点有关，且故障节点是主，我们需要更新Plan继续该slot迁移。（如果这个过程中又挂了怎么办？）
(a)RUNNING
(a)RUNNING -> (b)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER
(a)RUNNING -> (b)MIGRATING
(a)RUNNING

复杂场景1:
1. 正在扩容，数据迁移中
2. 某一台主机被写打爆了
处理方案：
1. 修改数据迁移方案，将那台机器上一部分热点slot迁移过去
2. 等待恢复
思考：
当压力降下来，服务恢复后，(b)状态就该取消了，因为之前建立的MigPlan已经被修改了，所以该结束迁移等待重新发起，就会出现这样的StateStack：
(a)RUNNING
(a)RUNNING -> (b)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER -> (d)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER
(a)RUNNING -> (b)MIGRATING(Resume后直接退出)
(a)RUNNING

2复杂场景2:
1. 正在扩容，数据迁移中
2. 某一台主机被写打爆了
3. 我们加了一台机器，做热点迁移
4. 迁移过程中，另一台机器挂了
处理方案：
1. 恢复新机器，重新选主，继续迁移
说明：
(a)RUNNING
(a)RUNNING -> (b)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER -> (d)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER -> (d)MIGRATING -> (e)WAITING_MASTER_FAILOVER
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER -> (d)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER
(a)RUNNING -> (b)MIGRATING(Resume后直接退出)
(a)RUNNING

复杂场景3，如果复杂场景2反过来呢:
1. 一台机器挂了，进入WAIT_FAILOVER状态
2. 另一台主机主机被打爆了
思考：
如果短时间内同时挂了两个节点，我们需要按先后顺序处理吗，可以同时处理吗？所以状态是整个集群的，还是每个节点的，似乎更像是节点粒度的？

(a)RUNNING
(a)RUNNING -> (b)WAIT_MASTER_FAILOVER

== 节点粒度状态
简单场景1:
1. 正在扩容，数据迁移中
2. 某台主机挂掉了
处理方案：
1. 人工确认进行AutoFailover
当Failover结束后，继续之前未完成的迁移，正常情况下会出现如下StateStack。但注意，之前的MigPlan已经无效了，因为故障节点就包含在其中，此时我们希望取消迁移。有两种情形需要考虑：
1. 如果Migrate被打断在slot迁移之间，可以直接结束。
2. 如果Migrate被打断在slot迁移之内，且该slot与故障节点无关，我们需要继续将该slot迁移完成，因为redis上还设置着迁移状态标记，并且同一个slot的数据分散在两台机器上。
3. 如果Migrate被打断在slot迁移之内，且该slot与故障节点有关，且故障节点是主，我们需要更新Plan继续该slot迁移。（如果这个过程中又挂了怎么办？）
(a)RUNNING
(a)RUNNING -> (b)MIGRATING
(a)RUNNING -> (b)MIGRATING -> (c)WAITING_MASTER_FAILOVER
(a)RUNNING -> (b)MIGRATING
(a)RUNNING

系统有一个ID->ServerState的结构，Server维护当前所有Server的状态，包括迁移状态和Failover状态，默认处于Running状态，除此还有一些其他标记状态。该结构与topo结构独立。
消息按Server粒度发送，如发现某个节点FAIL了，则想该ID发送消息。状态控制也是向ServerState发送。这样一来统一时间可能有多个Server处于WAIT_FAILOVER状态，管理员可以根据自己的判断选择优先处理哪个。
迁移可以按source并行化，系统中同时有多个MigrateTask多个FailoverTask，每个Server一个。

对于每个Server:
1. 如果处于RUNNING状态，收到migrate请求，进入MIGRATING状态，创建一个Migrate任务，进行迁移
2. 如果处于RUNNING状态，收到failure信息，进入WAIT_FAILOVER状态
3. 如果处于MIGRATING状态，收到failure信息，停止Migrate任务，进入WAIT_FAILOVER状态
4. 如果处于WAIT_FAILOVER状态，收到failover信息，则创建Failover任务进行处理，处理完回到RUNNING状态
5. 如果处于WAIT_FAILOVER状态，收到migrate请求，则创建Failover任务进行处理

"RUNNING",       MSG_MIGRATE,       "MIGRATING"
"RUNNING",       MSG_FAILURE,       "WAIT_FAILOVER"
"MIGRATING",     MSG_FAILOVER,      "WAIT_FAILOVER"
"MIGRATING",     MSG_MIGRATE_CACEL, "RUNNING"
"WAIT_FAILOVER", MSG_MIGRATE,       "MIGRATING"
"WAIT_FAILOVER", MSG_FAILOVER,      "RUNNING"
"ANY",           MSG_DISABLE_READ
"ANY",           MSG_DISABLE_WRITE
"ANY",           MSG_DISABLE_READWRITE

同一ReplicaSet：
1. 主(A)先挂了，选择(B)为新主
2. 从(B)后挂了

A -> WAIT_FAILOVER -> FAILOVER选择B为新主
B -> WAIT_FAILOVER -> 自动处理，但此时他认为自己是从

# 2015/03/02

## 场景

### 做迁移
- 向/api/migrate发送POST请求，传递SourceId, TargetId, SlotStart, SlotEnd
- 如果当前Controller不是Leader，返回错误（写一个gin中间件）
- 请求由FrontEnd.handleMigrate处理
- task := MigrateManager.FindMigrateTaskBySource(serverId string)
- 如果task不为空，表示有正在运行的迁移任务，返回迁移ID和迁移任务状态（RUNNING,PAUSED,STOP）
- 如果task没有正在运行，则创建一个：task := MigrateManager.CreateMigrateTask(migId, sourceId, targetId string , start, end int)
- 启动该task.Start()

```
POST /api/migrate/create
POST /api/migrate/canel
POST /api/migrate/reset
POST /api/migrate/pause
POST /api/migrate/autorebanlance
GET  /api/migrate/search?type=sourceid&id=$id
```

```golang
 type MigrateRequest struct {
      SourceId  string `json:"SourceId"`
      TargetId  string `json:"TargetId"`
      SlotStart int    `json:"SlotStart"`
      SlotEnd   int    `json:"SlotEnd"`
 }
```

### 迁移中Source节点挂了
- RemoteSpectator向/api/topo/snapshot发送region粒度的topo信息，其中包含有故障节点（某台Server状态是FAIL）
- 对于RegionToplogoy.AllServers()中的每个Server
- 判断Server状态，如果是FAIL状态server.Dead()，向StatManager.SendMessage(MSG_DEAD)
- 如果是非FAIL状态，向StatManager.SendMessage(MSG_ALIVE)

CASE 1:
1. 需要扩容，节点Ａ向节点Ｂ迁移数据
2. 正在扩容，（迁移任务Ｍ0）
3. 节点Ａ挂了

1. 查看迁移任务是否存在，不存在则创建，存在则返回失败
2. 发现存在，发送迁移取消命令取消Ｍ0
3. 再次创建迁移任务Ｍ1
4. 观察者发现Ａ处于FAIL状态
5. 将BAD状态告知Controller
6. 如果当前处于RUNNING状态，检查主从，和是否可以自动进行Failover，不可以的化，进入WAIT_FAILOVER状态
7. 自动Failover失败，进入WAIT_FAILOVER状态
8. 观察者持续发送FAIL状态，因为处于WAIT_FAILOVER_BEGIN状态，直接返回
9. 此时又出现NOT_BAD消息，设置状态为RUNNING，返回
10. 再次发现FAIL，发送BAD消息，进入WAIT_FAILOVER_BEGIN状态
11. 管理员发送FAILOVER消息，路由消息到该Server
12. 发现处于WAIT_FAILOVER状态，检查主从，执行Failover任务，进入WAIT_FAILOVER_END状态
13. FAILOVER状态仅当收到NOT_BAD消息才会回到RUNNING状态

BAD和NOT_BAD状态的定义，并非Server处于FAIL状态就是BAD，当Server处于FAIL状态，但已经被封禁了，其实此时是NOT_BAD状态

状态：迁移中，读封禁，写封禁，读写封禁，切换，正常

# 2015/03/03

(READ,WRITE,FAIL,ROLE,CMD)

# 2015/03/05

处理迁移任务，迁移只能针对Master进行，当节点角色变化时，需要更新迁移任务，因为迁移可能正处在中间状态（Slot内部）

消息

- MIGRATE_CREATE     POST /migrate
- MIGRATE_CANCEL     
- MIGRATE_PAUSE      
- MIGRATE_RESUME     
- MIGRATE_RESET      
- MIGRATE_CANCEL_ALL 

- SetAsMaster      POST /node/
- DisableRead       
- EnableRead        
- DisableWrite      
- EnableWrite
- Failover

# 2015/03/06
- 一个从在FAILOVER后，进入OFFLINE状态，如果重新启动起来，并不会自动进入RUNNING状态，因
  为rw是封禁的。一方面是由于我们可以手动做server的封禁，所以实际上我们允许出现正常运行的
  节点不对外提供服务，另一方面，一个server启动起来后在数据同步完之前，实际上是不应该提供
  服务的。这是一个管理上的注意事项。

go func() { c <- controller.ProcessCommand(cmd) }

select {
    case output := <-c:
    case <-timeout:
}


Controller同时是RoleSwitchListener和FailoverStateChangeListener

# 2015/03/07

Master故障，进行Failover之后，故障的节点仍然被标记为Master

# 2015/03/08

TODO:
- meta
- Master执行Failover过程中，暂停迁移任务，成功后fix迁移任务信息
- api/console

# 2015/03/09

streams该怎么做，rx的方式可用吗

# 2015/03/12

  - 为什么我们将状态简化成四个（本质上是三个）状态，主要原因是节点实际状态（主从，读写）是维护在
    Redis集群里，而非Zk等Controller需要控制的地方。

* 迁移问题：

  - 如果迁移的源或目标节点挂了，我们需要将迁移任务停止吗？

    比如Ａ向Ｂ迁移，极端情况下出现了网络划分，多数节点认为Ｂ挂了，但是Ａ和Ｂ之前网
    络是联通的，然后数据迁移过去了，但最终Ｂ会被Failover，刚才迁移的数据就丢了。这
    当然是很极端的情况。

  - 那么如果当目标节点不处于RUNNING状态就停止迁移，进入RUNNING状态就继续迁移呢？如果进行
    了角色切换呢？
    
  - 如果Controller挂了，如何恢复未完成的slot，需要持续检查[slot->nodeid]的状态，如果任务中没有，需要创建迁移任务，继续做完

* 迁移三种状态：RUNNING,PAUSE,ERROR

* RUNNING
  - IsMaster && FAIL 进入ERROR
  - IsSlave 找到主，进行切换
  - CMD_PAUSE，进入ERROR

* PAUSE
  - CMD_RESUME，进入RUNNING状态

* ERROR
  - IsMaster && not FAIL，进入RUNNING
  - CMD_PAUSE，进入PAUSE

fromNode := cluster.FindNode(task.From.Id)
toNode := cluster.FindNode(task.To.Id)
// if not exist, exception

# 2015/03/15

迁移任务逻辑似乎错了。

迁移过程中，如果节点挂了，在Failover之后，无论是源节点还是目标节点，slot信息其实都已经丢失了，除非我们缓存一份之前的拓扑关系，或者根据Range来查找故障节点属于哪个新节点。其实都不可靠。

如果简单一点，遇到故障时，直接取消该迁移任务，然后根据集群信息重建迁移任务。将问题的复杂性抛给迁移任务重建。

重建迁移任务的困难，先考虑目标节点挂了，有几种可能：
1. 目标挂了，进行了Failover，之后重新启动了
2. 目标挂了，进行了Failover，但一直未处理
3. 目标挂了，节点摘除
第一种情况节点会自动作为从加入到集群，好办。后两种情况，其实是丢失了信息的。

Case 0:
1. A向B迁移
2. A设置完Migrating状态，B挂了
3. 此时即便Controller发现A有[x->B]的迁移信息，我们也没有办法知道B的替换者是谁了，因为挂掉的节点，是不记录信息的

迁移过程中，Source节点挂了：
1. 因为Source的从也会设置Migrating标记，Failover后，仍可以看到，目标未变，可以恢复

迁移过程中，可能出现多出一个主的情况：
1. B挂了，但有迁移状态未广播出去
2. 选了B的从Ｃ做新主
3. B启动起来
此时就可能出现B和C都是主

难点就是如何避免在数据迁移过程中出现的各种问题。尤其是，目标节点挂了时候。

主地域所有节点挂了该如何恢复？只能将主搞到其他地域，做两次SetAsMaster切回去，这需要保证操作可靠，这本身就很难。

# 2015/03/16

- dirty_slots

  /* 1) If the sender of the message is a master, and we detected that
   *    the set of slots it claims changed, scan the slots to see if we
   *    need to update our configuration. */
  if (sender && nodeIsMaster(sender) && dirty_slots)
  clusterUpdateSlotsConfigWith(sender,senderConfigEpoch,hdr->myslots);

如果自己记录的slots信息和sender消息头中带来的slots不相同，说明sender的slots有变化，判断是否需要更新。

  /* The slot is in importing state, it should be modified only
   * manually via redis-trib (example: a resharding is in progress
   * and the migrating side slot was already closed and is advertising
   * a new config. We still want the slot to be closed manually). */
  if (server.cluster->importing_slots_from[j]) continue;

如果处于IMPORTING状态，不处理？这个概率还是很大的，是说在一个处于IMPORTING状态的目标节点挂掉之后，系统进行了选
主，当这个节点重新启动后，它会处于何种状态（变成从，但是还有IMPORTING状态的slot？），所以一个节点挂掉之后，最好
的办法是重新加一个，把挂掉的彻底删掉。

  /* 2) We also check for the reverse condition, that is, the sender
   *    claims to serve slots we know are served by a master with a
   *    greater configEpoch. If this happens we inform the sender.
   *
   * This is useful because sometimes after a partition heals, a
   * reappearing master may be the last one to claim a given set of
   * hash slots, but with a configuration that other instances know to
   * be deprecated. Example:
   *
   * A and B are master and slave for slots 1,2,3.
   * A is partitioned away, B gets promoted.
   * B is partitioned away, and A returns available.
   *
   * Usually B would PING A publishing its set of served slots and its
   * configEpoch, but because of the partition B can't inform A of the
   * new configuration, so other nodes that have an updated table must
   * do it. In this way A will stop to act as a master (or can try to
   * failover if there are the conditions to win the election). */

迁移操作步骤：
1. Mark, (SM,SS)->MIGRATING (TM)->IMPORTING
2. MigrateSlot until empty
3. setslot node
   - TM TS, SM, SS

# 2015/03/26

Controller集群化：

/r3/app/ksarch-test
{"master_region":"bj","regions":["bj","hz"],"appname":"ksarch-test","autofailover":true}

/r3/app/ksarch-test/controllers

/r3/app/ksarch-test/controllers/cc_bj_000000001
{"ip":"127.0.0.1","http_port":7000,"ws_port":8001,"region":"bj"}

全局Failover控制

Failover进行条件：
1. 当前没有其他正在进行的Failover
2. 最近30分钟内没有进行过Failover
该过程如何实现

/r3/failover/doing
/r3/failover/history

初始化
1. 根据appName去zk上获取appMetaData，如果为空则Fatal
2. 注册Controller数据到zk，进行选主（ClusterLeader和RegionLeader）
3. ClusterLeader负责处理消息，RegionLeader负责发送快照数据
4. 获取LeaderConfig

操作：
1. IsRegionLeader()
2. IsClusterLeader()
3. ClusterLeaderHttpAddress()

watchon:
/r3/failover/doing
/r3/failover/history
/r3/app/ksarch-test/controller
/r3/app/ksarch-test/controller/leader

# 2015/03/28

UI需要展示的内容：
1. 集群Topo （地域，分片，主从，slots）
2. AppConfig (AutoFailover,MasterRegion,Regions)
3. Log ()
4. Operation Feeds
5. Migration Task
6. Rebalance Task

问题是这么多信息（如128个分片，数十个迁移任务)，如何合理的显示出来，另外如何维护React的状态，避免消息量过大导致UI很卡。

我们平时关注什么：
1. 集群拓扑和节点状态，故障处理
2. qps, mem，同步等基本信息
3. 数据迁移，扩容，缩容

几个展示区域：
1. 新节点（Master,NotDead,NoSlave,NoSlot），操作有：设置SlaveOf、组合ReplicaSet，下线
2. 集群拓扑区域
3. 迁移任务区域
4. Rebalance任务区域
5. 日志
6. 操作Feed流
7. 事件Feed流
